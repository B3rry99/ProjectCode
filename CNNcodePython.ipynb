{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('omw-1.4')\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, Flatten, MaxPooling1D\n",
    "import numpy as np\n",
    "from keras.preprocessing import text\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e7ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in Airlines data\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\AirlinesData.csv\")\n",
    "data.drop(data.columns[[0,3,4,5,6,7,8,9,11,12,13,14]], axis=1, inplace=True)\n",
    "data = data[data.airline_sentiment_confidence == 1.0]\n",
    "data = data[data.airline_sentiment != \"neutral\"]  # removing neutral sentiment rows\n",
    "def preprocess(text):\n",
    "    preprocessed_text = []\n",
    "    for t in text.split():\n",
    "        if len(t) > 1:\n",
    "            t = '' if t[0] == '@' and t.count('@') == 1 else t  # removing links and mentions\n",
    "            t = '' if t.startswith('http') else t\n",
    "            #t = '' if t.startswith('#') else t\n",
    "        preprocessed_text.append(t)\n",
    "    return ' '.join(preprocessed_text)\n",
    "Cleaned_Tweet = []\n",
    "for tweet in data[\"text\"]:\n",
    "    cleaned = preprocess(tweet)\n",
    "    tokenizedtweet = tweet.split()  # Tokenizing tweets\n",
    "    #tweetNoStopWords = [word for word in tokenizedtweet if word not in stopwords.words('english')] # Removing stopwords\n",
    "    #cleaned = ' '.join(tweetNoStopWords) # Joining tokenized tweet\n",
    "    cleaned = cleaned.translate(str.maketrans('', '', string.punctuation)) # Removing punctuation\n",
    "    #cleaned = ''.join([i for i in cleaned if not i.isdigit()]) # removing digits\n",
    "    Cleaned_Tweet.append(cleaned)\n",
    "\n",
    "# producing test/training data \n",
    "data[\"Cleaned_Tweet\"] = Cleaned_Tweet\n",
    "dataShuffled = data.sample(frac=1)\n",
    "ShuffledTweets = []\n",
    "for tweet in dataShuffled[\"Cleaned_Tweet\"]:\n",
    "    ShuffledTweets.append(tweet)\n",
    "ShuffledSenti = []\n",
    "for senti in dataShuffled[\"airline_sentiment\"]:\n",
    "    ShuffledSenti.append(senti)    \n",
    "train_x = ShuffledTweets[:7118] # list of cleaned tweets\n",
    "test_x = ShuffledTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    #tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "TweetsJoined = ' '.join(train_x) \n",
    "TweetsJoined = clean_doc(TweetsJoined)\n",
    "TweetsJoined = ' '.join(TweetsJoined)\n",
    "text_file = open(r'C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\AirlinesTest.txt', 'w')\n",
    "text_file.write(TweetsJoined)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename = 'C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\AirlinesTest.txt' # Must convert all airline tweets into single text file\n",
    "text = load_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab # use this function on txt file of ALL airline tweets\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "add_doc_to_vocab(filename,vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "min_occurane = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "#print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393358a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440318b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Training model ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc2(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    #table = str.maketrans('', '', punctuation)\n",
    "    #tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "CleanedTweets = []\n",
    "for tweet in train_x:\n",
    "    Cleaned = clean_doc2(tweet,vocab)\n",
    "    CleanedTweets.append(Cleaned)\n",
    "train_docs = CleanedTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "# pad sequences\n",
    "max_length2 = max([len(s.split()) for s in ShuffledTweets])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length2, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = []\n",
    "for value in train_y:\n",
    "    if value == \"positive\":\n",
    "        train_Y.append(value.replace(\"positive\",\"1\"))\n",
    "        \n",
    "    else:\n",
    "        train_Y.append(value.replace(\"negative\",\"0\"))\n",
    "test_Y = []\n",
    "for value in test_y:\n",
    "    if value == \"positive\":\n",
    "        test_Y.append(value.replace(\"positive\",\"1\"))\n",
    "        \n",
    "    else:\n",
    "        test_Y.append(value.replace(\"negative\",\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = test_x\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length2, padding='post')\n",
    "ytrain = []\n",
    "for value in train_Y:    \n",
    "    ytrain.append(int(value))\n",
    "ytrain = np.array(ytrain)\n",
    "\n",
    "ytest = []\n",
    "for value in test_Y:    \n",
    "    ytest.append(int(value))\n",
    "ytest = np.array(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length2))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8baf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(string):\n",
    "    return string.encode('ascii', errors='ignore').decode()\n",
    "# Returns list of predictions (0 = negative, 1 = positive)\n",
    "def predict_sentiment(tweets, vocab, tokenizer, model):\n",
    "    # clean\n",
    "    cleanedTweets = []\n",
    "    for tweet in tweets:\n",
    "        tokens = clean_doc(tweet)\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        line = ' '.join(tokens)\n",
    "        line = remove_non_ascii(line)\n",
    "        cleanedTweets.append(line)\n",
    "    \n",
    "    # convert to line\n",
    "    \n",
    "    # encode\n",
    "    #line = tokenizer.fit_on_texts(line)\n",
    "    encoded_doc = tokenizer.texts_to_sequences(cleanedTweets)\n",
    "    # padding\n",
    "    paddedData = pad_sequences(encoded_doc, maxlen=max_length2, padding='post')\n",
    "    # predict\n",
    "    pred = model.predict(paddedData, verbose=0)\n",
    "    y = pred.tolist()\n",
    "    predictions = []\n",
    "    for value in y:\n",
    "        predictions.append(round(value[0]))\n",
    "    \n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a90d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26279046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "data1 = pd.read_csv(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\hashtag_donaldtrump.csv\")\n",
    "data2 = pd.read_csv(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\hashtag_joebiden.csv\")\n",
    "\n",
    "data = data1.append(data2, ignore_index=True)\n",
    "# Removing irrelevant columns\n",
    "data.drop(data.columns[[1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]], axis=1, inplace=True)\n",
    "\n",
    "#converting all tweets to strings\n",
    "data['tweet']=data['tweet'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    preprocessed_text = []\n",
    "    for t in text.split():\n",
    "        if len(t) > 1:\n",
    "            t = '' if t[0] == '@' and t.count('@') == 1 else t\n",
    "            t = '' if t.startswith('http') else t\n",
    "            #t = '' if t.startswith('#') else t\n",
    "        preprocessed_text.append(t)\n",
    "    return ' '.join(preprocessed_text)\n",
    "Cleaned_Tweet =[]\n",
    "for tweet in data[\"tweet\"]:\n",
    "    Cleaned_Tweet.append(preprocess(tweet))\n",
    "data[\"Cleaned_Tweet\"] = Cleaned_Tweet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73888d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new column with hashtags contained in tweet\n",
    "data['hashtag'] = data['tweet'].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "# creating list of all hashtags used\n",
    "hashtags = []\n",
    "for i in data['hashtag']:\n",
    "    hashtags.append(i)\n",
    "\n",
    "flat_list = [item for sublist in hashtags for item in sublist]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(topic, topicdf):\n",
    "    # Creating dataframe containing Trump related tweets and their creation date        \n",
    "    topic = []\n",
    "    topic = pd.DataFrame(topic)\n",
    "    topic[\"created\"] = created\n",
    "    topic[\"tweet\"] = tweet\n",
    "\n",
    "    # Creating new dataframe for Sentiment analysis results sorted by date\n",
    "    topicdf = []\n",
    "    topicdf = pd.DataFrame(topicdf)\n",
    "    for date in dates:\n",
    "        tweets = []\n",
    "        for i in range(len(topic[\"tweet\"])):\n",
    "            if topic[\"created\"][i].startswith(date):\n",
    "                tweets.append(topic[\"tweet\"][i])\n",
    "        #Scaling down tweets to only analyse ~10,000\n",
    "        random.shuffle(tweets)\n",
    "        tweets = tweets[:1000]\n",
    "        startTime = time.time()\n",
    "        predictions = predict_sentiment(tweets,vocab,tokenizer,model) # creating predictions\n",
    "        executionTime = (time.time() - startTime)\n",
    "        print('Execution time in seconds: ' + str(executionTime))\n",
    "        Neg = predictions.count(0)\n",
    "        Pos = predictions.count(1)\n",
    "        values = [Neg,Pos]\n",
    "        topicdf[date] = values\n",
    "        print(date)\n",
    "    return topicdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31865fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating tweets based on hashtag and performing sentiment analysis\n",
    "# Analysis for Trump related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Trump\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"trump\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"DonaldTrump\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"donaldtrump\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Trumptotaltweets = predict(\"Trump\",\"Trumptotaltweets\")\n",
    "# Analysis for Biden related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Biden\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"biden\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"JoeBiden\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"joebiden\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Bidentotaltweets = predict(\"Biden\",\"Bidentotaltweets\")\n",
    "# Analysis for Elections2020 related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Elections2020\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"USAElections2020\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"USElections2020\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Elections2020totaltweets = predict(\"Elections2020\",\"Elections2020totaltweets\")            \n",
    "# Analysis for KamalaHarris related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"KamalaHarris\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"Harris\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"harris\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "KamalaHarristotaltweets = predict(\"KamalaHarris\",\"KamalaHarristotaltweets\")            \n",
    "# Analysis for MAGA related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"MAGA\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"MAGA2020\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"maga\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "MAGAtotaltweets = predict(\"MAGA\",\"MAGAtotaltweets\")            \n",
    "# Analysis for COVID19  related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"COVID19\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"covid19\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"coronavirus\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"Coronavirus\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"COVID\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i]) \n",
    "COVIDtotaltweets = predict(\"COVID\",\"COVIDtotaltweets\")\n",
    "# Analysis for TrumpMeltdown  related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"TrumpMeltdown\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"trumpmeltdown\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])         \n",
    "TrumpMeltdowntotaltweets = predict(\"TrumpMeltdown\",\"TrumpMeltdowntotaltweets\")\n",
    "# Analysis for Democrats  related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Democrats\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"democrats\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])                    \n",
    "Democratstotaltweets = predict(\"Democrats\",\"Democratstotaltweets\")\n",
    "# Analysis for GOP  related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"GOP\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"GOP2020\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])         \n",
    "        elif hashtag == \"gop\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])         \n",
    "GOPtotaltweets = predict(\"GOP\",\"GOPtotaltweets\")\n",
    "# Analysis for Pennsylvania related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Pennsylvania\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"pennsylvania\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Pennsylvaniatotaltweets = predict(\"Pennsylvania\",\"Pennsylvaniatotaltweets\")\n",
    "# Analysis for Obama  related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Obama\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"obama\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])            \n",
    "Obamatotaltweets = predict(\"Obama\",\"Obamatotaltweets\")\n",
    "# Analysis for Michigan related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Michigan\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"michigan\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i]) \n",
    "Michigantotaltweets = predict(\"Michigan\",\"Michigantotaltweets\")            \n",
    "# Analysis for VoteHimOut related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"VoteHimOut\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"votehimout\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "VoteHimOuttotaltweets = predict(\"VoteHimOut\",\"VoteHimOuttotaltweets\")\n",
    "# Analysis for CNN related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"CNN\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"cnn\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "CNNtotaltweets = predict(\"CNN\",\"CNNtotaltweets\")            \n",
    "# Analysis for Rebulicans related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Republicans\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"republicans\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])            \n",
    "Rebulicanstotaltweets = predict(\"Rebulicans\",\"Rebulicanstotaltweets\")            \n",
    "# Analysis for Florida related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Florida\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"florida\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Floridatotaltweets = predict(\"Florida\",\"Floridatotaltweets\")\n",
    "# Analysis for Georgia related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Georgia\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"georgia\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Georgiatotaltweets = predict(\"Georgia\",\"Georgiatotaltweets\")\n",
    "# Analysis for China related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"China\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"china\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Chinatotaltweets = predict(\"China\",\"Chinatotaltweets\")\n",
    "# Analysis for FoxNews related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"FoxNews\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"foxnews\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "FoxNewstotaltweets = predict(\"FoxNews\",\"FoxNewstotaltweets\")\n",
    "# Analysis for Pence related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"Pence\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"pence\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "Pencetotaltweets = predict(\"Pence\",\"Pencetotaltweets\")\n",
    "# Analysis for TrumpIsLosing related tweets\n",
    "created = []\n",
    "tweet = []\n",
    "for i in range(len(data[\"tweet\"])):\n",
    "    for hashtag in data[\"hashtag\"][i]:\n",
    "        if hashtag == \"TrumpIsLosing\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "        elif hashtag == \"trumpislosing\":\n",
    "            created.append(data[\"created_at\"][i])\n",
    "            tweet.append(data[\"Cleaned_Tweet\"][i])\n",
    "TrumpIsLosingtotaltweets = predict(\"TrumpIsLosing\",\"TrumpIsLosingtotaltweets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics = [TrumptotaltweetsCNN,BidentotaltweetsCNN,Elections2020totaltweetsCNN,KamalaHarristotaltweetsCNN,MAGAtotaltweetsCNN,\n",
    "          COVIDtotaltweetsCNN,TrumpMeltdowntotaltweetsCNN,DemocratstotaltweetsCNN,GOPtotaltweetsCNN,PennsylvaniatotaltweetsCNN,\n",
    "          ObamatotaltweetsCNN,MichigantotaltweetsCNN,VoteHimOuttotaltweetsCNN,CNNtotaltweetsCNN,RepublicanstotaltweetsCNN,\n",
    "          FloridatotaltweetsCNN,GeorgiatotaltweetsCNN,ChinatotaltweetsCNN,FoxNewstotaltweetsCNN,PencetotaltweetsCNN,TrumpIsLosingtotaltweetsCNN]\n",
    "#Calculating sentiment score for each topic sorted by date\n",
    "dates = [\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\"]\n",
    "#Calculating sentiment score for each topic sorted by date\n",
    "FinalScores =[]\n",
    "for topic in Topics:    \n",
    "    Scores = []\n",
    "    for date in dates:\n",
    "        SentiScore = topic[date][1]-topic[date][0]\n",
    "        SentiScore = SentiScore/(topic[date][1]+topic[date][0]) \n",
    "        Scores.append(SentiScore)\n",
    "    FinalScores.append(Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data\n",
    "TrumptotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Trump_CNN.pkl\")\n",
    "BidentotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Biden_CNN.pkl\")\n",
    "Elections2020totaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Elections2020_CNN.pkl\")\n",
    "KamalaHarristotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\KamalaHarris_CNN.pkl\")\n",
    "MAGAtotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\MAGA2020_CNN.pkl\")\n",
    "COVIDtotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\COVID_CNN.pkl\")\n",
    "TrumpMeltdowntotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\TrumpMeltdown_CNN.pkl\")\n",
    "DemocratstotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Democrats_CNN.pkl\")\n",
    "GOPtotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\GOP_CNN.pkl\")\n",
    "PennsylvaniatotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Pennsylvania_CNN.pkl\")\n",
    "ObamatotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Obama_CNN.pkl\")\n",
    "MichigantotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Michigan_CNN.pkl\")\n",
    "VoteHimOuttotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\VoteHimOut_CNN.pkl\")\n",
    "CNNtotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\CNN_CNN.pkl\")\n",
    "RepublicanstotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Republicans_CNN.pkl\")\n",
    "FloridatotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Florida_CNN.pkl\")\n",
    "GeorgiatotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Georgia_CNN.pkl\")\n",
    "ChinatotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\China_CNN.pkl\")\n",
    "FoxNewstotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\FoxNews_CNN.pkl\")\n",
    "PencetotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\Pence_CNN.pkl\")\n",
    "TrumpIsLosingtotaltweetsCNN.to_pickle(\"C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\SentiData\\\\TrumpIsLosing_CNN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ab5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating sentiment data\n",
    "Topicnames = [\"Trump\",\"Biden\",\"Elections2020\",\"KamalaHarris\",\"MAGA\",\n",
    "          \"COVID\",\"TrumpMeltdown\",\"Democrats\",\"GOP\",\"Pennsylvania\",\n",
    "          \"Obama\",\"Michigan\",\"VoteHimOut\",\"CNN\",\"Republicans\",\n",
    "          \"Florida\",\"Georgia\",\"China\",\"FoxNews\",\"Pence\",\"TrumpIsLosing\"]\n",
    "SentimentData = pd.DataFrame(FinalScores[0])\n",
    "for i in range(0,21):\n",
    "    SentimentData[i] = FinalScores[i]\n",
    "index = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "for i in range(0,21):\n",
    "    int2str = str(index[i])\n",
    "    SentimentData.rename({index[i]: Topicnames[i]}, axis=1, inplace=True)\n",
    "SentimentData.rename({21: \"TrumpIsLosing\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c42b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to csv file for use in R\n",
    "SentimentData.to_csv('C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\SentimentDataCNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing TweetEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ed963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading testing data + labels\n",
    "with open('C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\TweetEvalTest.txt', encoding=\"utf8\") as f:\n",
    "    TestData = f.readlines()\n",
    "with open('C:\\\\Users\\\\Jack\\\\Desktop\\\\Uni Work\\\\PROJECT\\\\Data\\\\TweetEvalTestLabels.txt', encoding=\"utf8\") as f:\n",
    "    TestLabels = f.readlines()\n",
    "TestData_ = []\n",
    "TestLabels_ = []\n",
    "for text in TestData:\n",
    "    TestData_.append(text.replace(\"\\n\",\"\"))\n",
    "for text in TestLabels:\n",
    "    text.replace(\"\\n\",\"\")\n",
    "    TestLabels_.append(int(text))\n",
    "TweetEvalTest = []\n",
    "TweetEvalTest = pd.DataFrame(TweetEvalTest)      \n",
    "TweetEvalTest[\"text\"] = TestData_\n",
    "TweetEvalTest[\"labels\"] = TestLabels_\n",
    "TweetEvalTest = TweetEvalTest[TweetEvalTest.labels != 1]\n",
    "Labels = []\n",
    "for value in TweetEvalTest[\"labels\"]:\n",
    "    if value == 2:\n",
    "        Labels.append(value-1)\n",
    "        \n",
    "    else:\n",
    "        Labels.append(value)\n",
    "cleanedTest = []\n",
    "for text in TweetEvalTest[\"text\"]:\n",
    "    text.replace(\"@user\",\"\").replace(\"http\",\"\")\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    cleanedTest.append(text)\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    #tokens = [word for word in tokens if word.isalpha()]\n",
    "Labels = np.array(Labels)\n",
    "encoded_doc = tokenizer.texts_to_sequences(cleanedTest)\n",
    "paddedData = pad_sequences(encoded_doc, maxlen=max_length2, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TweetEvalTest # 0=neg, 1=neutral, 2=pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(paddedData, Labels, verbose=0) # testing accuracy for TweetEval for CNN\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37d714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5740d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f5df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3a701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
